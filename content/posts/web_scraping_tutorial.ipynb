{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Web Scraping with Python: An Introductory Tutorial\n",
    "\n",
    "#### Rob Osterburg \n",
    "##### Software Engineer / Instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "* The generous [sponsors](http://denverdatascienceday.com/index.php/sponsors/) of [Denver Data Science Day 2017](http://denverdatascienceday.com/)\n",
    "\n",
    "* [Galvanize](https://www.galvanize.com/pick-a-location?page=%2F) for hosting [Denver Data Science Day 2017](http://denverdatascienceday.com/)\n",
    "\n",
    "* Bob Mickus, Tyler B. and all the volunteers from [PyData Denver](https://www.meetup.com/PyData-Denver/) who organized [Denver Data Science Day](http://denverdatascienceday.com/)\n",
    "\n",
    "* Miguel Grinberg whose [Easy Web Scraping with Python](https://blog.miguelgrinberg.com/post/easy-web-scraping-with-python) blog post inspired this tutorial.  Miguel's posted his tutorial in 2014 and PyVideo.org has recently undergone a significant revision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics\n",
    "* Which Python packages to use\n",
    "* Cover key concepts, tools and techniques\n",
    "* Scrape some data \n",
    "    - Organize it using a namedtuple\n",
    "    - Persist it as a JSON file\n",
    "    - Read it back into a namedtuple and demonstrate semantic equivilence \n",
    "* Share recommended resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Packages\n",
    "\n",
    "* requests -- Issues HTTP requests to web servers and handles the response \n",
    "\n",
    "* beautifulsoup4 -- Parse the returned web page and makes it searchable\n",
    "\n",
    "* These packages are *not* in Python's standard library, here is how to install them:\n",
    " \n",
    "    - Standard Python: `pip install requests beautifulsoup4`\n",
    "\n",
    "    - Anaconda Python: `conda install requests beautifulsoup4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Requests is 'HTTP for Humans'\n",
    "import requests\n",
    "\n",
    "# BeautifulSoup parses and builds tree from HTML\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warning! Warning!\n",
    "* Only scrape as a last resort, first see if the site has an API or other means of accessing their data\n",
    "* Web scraping is commonly frowned upon by the site's owners\n",
    "* **Always check the site's Terms of service, Conditions of use, and robots.txt file**   \n",
    "* Aggressive scrapers can take a site down, owner's can rightfully consider that a denial of service attack\n",
    "* If in doubt, talk to a lawyer, especially for *anything* work related\n",
    "* Watch [Sustainable Scrapers, PyData DC, 2016](http://pyvideo.org/pydata-dc-2016/sustainable-scrapers.html) to understand how a data journalist deals with these issues. It's a good talk and quite entertaining -- in a geeky way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyVideo.org -- Is it OK to scrape?\n",
    "* [PyVideo.org](http://pyvideo.org) provides valuable meta-data about Python presentations\n",
    "* It makes finding talks easy and can be searched by event, presenter, or topic \n",
    "* But what restrictions does it have against scraping?  \n",
    "* Let's take a look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyVideo.org - Terms and Conditions?\n",
    "\n",
    "None that I can see, the site's [About page](http://pyvideo.org/pages/about.html) says:\n",
    "\n",
    "> ... PyVideo.org is a freely available index of freely available resources that seek to provide everyone with the opportunity to learn about Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyVideo.org - Robots.txt?\n",
    "* A robots.txt files contain *restrictions* on the content that web crawlers are permitted to access \n",
    "\n",
    "* The [http://pyvideo.org/robots.txt](http://pyvideo.org/robots.txt) is empty, except for one comment \n",
    "\n",
    "* It appears that there are no restrictions on scraping PyVideo.org  \n",
    "\n",
    "* Let's minimize our impact on the site \n",
    "\n",
    "* To see *The Robots Exclusion Protocol* visit [http://www.robotstxt.org/](http://www.robotstxt.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200\n"
     ]
    }
   ],
   "source": [
    "# Getting the Page\n",
    "resp = requests.get('http://pyvideo.org/tags.html')\n",
    "\n",
    "# A status code other than 200 indicates problem of some sort\n",
    "print('status code:', resp.status_code)\n",
    "\n",
    "# A better approach is let requests throw an exception when a problem occurs\n",
    "resp.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HTTP Responses\n",
    "\n",
    "HTTP Verb | Effect | Success | Failure\n",
    "--------- | ------ | ------- | --------\n",
    "POST      | Create | 200     | 400, 40X, 500\n",
    "**GET**       | **Read**   | **200**     | **400, 40X, 500**\n",
    "PUT       | Update | 200     | 400, 40X, 500 \n",
    "DELETE    | Delete | 200     | 400, 40X, 500\n",
    "\n",
    "* [Comprehensive list of HTTP status codes](https://httpstatuses.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the Page\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## BeautifulSoup - Parses the page\n",
    " \n",
    "* Models the page as a tree\n",
    "* Similar to the Document Object Model\n",
    " ![tags page structure](./images/htmltree.png)\n",
    "\n",
    "* Parsers build the tree, pick one\n",
    " - html.parser -> Default parser for Python 3\n",
    " - HTMLParser  -> Default parser for Python 2\n",
    " - lxml        -> Fast requires installing a C library and a PyPI package \n",
    " - html5lib    -> Pure Python and part of the standard library\n",
    " \n",
    " cite: https://interactivepython.org/runestone/static/pythonds/Trees/ExamplesofTrees.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BeautifulSoup - Well documented and easy to use API\n",
    "* [select](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors)('css selector') --> [List of Tags]\n",
    "\n",
    "* [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all)(tags, keyword_args, attrs={'attr', 'value'})  --> [List of Tags]\n",
    "    \n",
    "* [find](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find)(tags, keyword_args, attrs={'attr', 'value'}) --> Tag\n",
    "\n",
    "* [BeautifulSoup 4 Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding Page Structure\n",
    "\n",
    "* To extract data from a page, you need to understand its structure\n",
    "* BeautifulSoup holds the tree in memory\n",
    "* Look into it using the our browser's built-in developer tools\n",
    "    - Mac -- Cmd + Opt + i\n",
    "    - Linux -- Ctrl + Shift + i\n",
    "    - Windows -- Ctrl + Shift + i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visiting the [Tags page](http://pyvideo.org/tags) you can see the structure of the page  \n",
    "![tags page structure](./images/structure-of-tags-page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Developer Tools - Getting a CSS selector\n",
    "* Use the element inspector to locate items you want to scrape\n",
    "* Right-click on the element > Copy > Copy CSS Selector\n",
    "* Paste the selector into your code\n",
    "* Make it more general by removing aspects that specific to *a* particular tag\n",
    "* In this case, `nth-child()` and `.col-md-3`\n",
    "* The idea here is to tune the selector to the data you want to scrape\n",
    "* Tutorial: [An Intro to CSS: Finding CSS Selectors](https://dailypost.wordpress.com/2013/07/25/css-selectors/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://pyvideo.org/index.html\"><i class=\"fa fa-fw fa-home\"></i> <span>Start</span></a>,\n",
       " <a href=\"http://pyvideo.org/events.html\"><i class=\"fa fa-fw fa-list-ul\"></i> <span>Events</span></a>,\n",
       " <a href=\"http://pyvideo.org/tags.html\"><i class=\"fa fa-fw fa-tags\"></i> <span>Tags</span></a>,\n",
       " <a href=\"http://pyvideo.org/speakers.html\"><i class=\"fa fa-fw fa-users\"></i> <span>Speakers</span></a>,\n",
       " <a href=\"http://pyvideo.org/pages/about.html\"><i class=\"fa fa-fw fa-info\"></i> <span>About</span></a>,\n",
       " <a href=\"http://pyvideo.org/pages/thank-you-contributors.html\"><i class=\"fa fa-fw fa-info\"></i> <span>Thank You</span></a>,\n",
       " <a href=\"http://pyvideo.org/pages/thanks-will-and-sheila.html\"><i class=\"fa fa-fw fa-info\"></i> <span></span></a>,\n",
       " <a href=\"http://pyvideo.org/tag/2to3/\">2to3</a>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a list of anchor elements using a CSS selector\n",
    "# First we need to clean our CSS using a for-loop\n",
    "topics = []\n",
    "as_copied_css = 'li.col-md-3:nth-child(1174) > a:nth-child(1)'\n",
    "cleaned_css = 'li > a'\n",
    "for topic in soup.select(cleaned_css):\n",
    "    topics.append(topic)\n",
    "topics[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why List Comprehensions?\n",
    "* List comprehensions are a powerful way to create lists\n",
    "* For-loop code above == List comprehension below\n",
    "* The CSS selector got us 90% of the way there\n",
    "* List comprehension can do the rest\n",
    "* I find them very handy, I think you will too\n",
    "* Tutorial: [Understanding List Comprehensions in Python 3](https://www.digitalocean.com/community/tutorials/understanding-list-comprehensions-in-python-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://pyvideo.org/index.html\"><i class=\"fa fa-fw fa-home\"></i> <span>Start</span></a>,\n",
       " <a href=\"http://pyvideo.org/events.html\"><i class=\"fa fa-fw fa-list-ul\"></i> <span>Events</span></a>,\n",
       " <a href=\"http://pyvideo.org/tags.html\"><i class=\"fa fa-fw fa-tags\"></i> <span>Tags</span></a>,\n",
       " <a href=\"http://pyvideo.org/speakers.html\"><i class=\"fa fa-fw fa-users\"></i> <span>Speakers</span></a>,\n",
       " <a href=\"http://pyvideo.org/pages/about.html\"><i class=\"fa fa-fw fa-info\"></i> <span>About</span></a>,\n",
       " <a href=\"http://pyvideo.org/pages/thank-you-contributors.html\"><i class=\"fa fa-fw fa-info\"></i> <span>Thank You</span></a>,\n",
       " <a href=\"http://pyvideo.org/pages/thanks-will-and-sheila.html\"><i class=\"fa fa-fw fa-info\"></i> <span></span></a>,\n",
       " <a href=\"http://pyvideo.org/tag/2to3/\">2to3</a>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's using a list comprehension \n",
    "# to build the list of links\n",
    "topics = [topic                                        # adds the topic to the list\n",
    "          for topic in soup.select('li > a')]          # loop over Tags from select\n",
    "topics[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Now lets look at one of the elements in our list\n",
    "* Each element is a BeautifulSoup Tag object\n",
    "* *Tag* allows access to all the element's data including\n",
    "    - *attributes* that are accessed using dictionary like syntax\n",
    "    - *contents* that are displayed on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(bs4.element.Tag,\n",
       " <a href=\"http://pyvideo.org/tag/2to3/\">2to3</a>,\n",
       " ['2to3'],\n",
       " 'http://pyvideo.org/tag/2to3/')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look a Tag instance\n",
    "topic = topics[7]\n",
    "# Access the data it holds\n",
    "type(topic), topic, topic.contents, topic['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* List comprehensions are concise and powerful to process data\n",
    "* From the list of anchor elements\n",
    "* Let's create a list of links to topics pages\n",
    "* Filtering out all unrelated links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1461,\n",
       " str,\n",
       " ['http://pyvideo.org/tag/2to3/',\n",
       "  'http://pyvideo.org/tag/3d/',\n",
       "  'http://pyvideo.org/tag/3d-printing/',\n",
       "  'http://pyvideo.org/tag/abc/',\n",
       "  'http://pyvideo.org/tag/accelerate/'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting and filtering the links\n",
    "topics = [topic['href']                       # extract attribute value\n",
    "          for topic in soup.select('li > a')  # loop over anchors\n",
    "          if '/tag/' in topic['href']]        # filter\n",
    "         \n",
    "len(topics), type(topics[0]), topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the page\n",
    "resp = requests.get('http://pyvideo.org/tag/scraping')\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the page\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Tag.find_all()\n",
    "* We have used `select()`, not let's try out `find_all()` \n",
    "* Our goal is create a list of the links to all the talks on a particular topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h4 class=\"entry-title\">\n",
       "<a href=\"/pydata-dc-2016/open-data-dashboards-python-web-scraping.html\" rel=\"bookmark\" title=\"Permalink to Open Data Dashboards &amp; Python Web Scraping\">\n",
       "           Open Data Dashboards &amp; Python Web Scraping\n",
       "        </a>\n",
       "</h4>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the video page links using find_all()\n",
    "h4_tags = soup.find_all('h4', class_='entry-title')\n",
    "h4_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BeautifulSoup in action\n",
    "* Methods: `find_all()` and `select()` both return list of Tag objects\n",
    "* Accessing descendents:  Tag.Descendent_Tag['attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, '/pydata-dc-2016/open-data-dashboards-python-web-scraping.html')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the link from <a> tag inside the <h4>\n",
    "findall_talks = [tag.a['href'] for tag in h4_tags]\n",
    "len(findall_talks), findall_talks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " 'http://pyvideo.org/pydata-dc-2016/open-data-dashboards-python-web-scraping.html')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same thing using select() -- In this case select seems cleaner\n",
    "site_url = 'http://pyvideo.org'\n",
    "select_talks = [site_url + tag['href'] \n",
    "                for tag in soup.select('article > section > h4 > a')]\n",
    "len(select_talks), select_talks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving on to extract presentation meta data\n",
    "* Our ultimate destination is now in sight -- The video's presentation page\n",
    "* It's the talk page -- that's where the best data are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get a page and parse it \n",
    "resp = requests.get(select_talks[0])\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Open Data Dashboards & Python Web Scraping'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the talk's title\n",
    "titles = [title.contents[0].strip() \n",
    "          for title in soup.select('.entry-title > a')]\n",
    "title = titles[0]\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marie Whittaker']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the speaker's name \n",
    "names = [elem.contents[0] \n",
    "         for elem in soup.select('.url')]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://pyvideo.org/events/pydata-dc-2016.html\">PyData DC 2016</a>,\n",
       " <a href=\"https://www.youtube.com/watch?v=kc676iLvib8\" rel=\"external\">YouTube</a>,\n",
       " <a href=\"http://pyvideo.org/tag/data/\">Data</a>,\n",
       " <a href=\"http://pyvideo.org/tag/scraping/\">scraping</a>,\n",
       " <a href=\"http://pyvideo.org/tag/web/\">web</a>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the talk details\n",
    "details = [detail \n",
    "           for detail in soup.select('.details-content > ul > li > a')]\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the YouTube link\n",
    "links = [detail['href'] \n",
    "         for detail in details \n",
    "         if 'youtube.com' in detail['href']]\n",
    "link = links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'scraping', 'web']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the subject tags\n",
    "tags = [link.contents[0].lower() \n",
    "        for link in details \n",
    "        if 'tag' in link['href']]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyData DC 2016\n",
      "\n",
      "Distilling a world of data down to a few key indicators can be an effective way of keeping an audience informed, and this concept is at the heart of a good dashboard. This talk will cover a few methods of scraping and reshaping open data for dashboard visualization, to automate the boring stuff so you have more time and energy to focus on the analysis and content.\n",
      "\n",
      "This talk will cover a basic scenario of curating open data into visualizations for an audience. The main goal is to automate data scraping/downloading and reshaping. I use python to automate data gathering, and Tableau and D3 as visualization tools -- but the process can be applied to numerous analytical/visualization suites.\n",
      "\n",
      "I'll discuss situations where a dashboard makes sense (and when one doesn't). I will make a case also that automation makes for a more seamless data gathering and updating process, but not always for smarter data analysis.\n",
      "\n",
      "Some python packages I'll cover for web scraping and downloading/reshaping open data include: openpyxl, pandas, xlsxwriter, and BeautifulSoup. I'll also touch on APIs.\n"
     ]
    }
   ],
   "source": [
    "# Extract the description\n",
    "paragraphs = soup.select('.entry-content > p')\n",
    "description = '\\n\\n'.join([p.contents[0] for p in paragraphs])\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Saving the Data\n",
    "\n",
    "* Use namedtuple to capture information about PyVideo talk\n",
    "\n",
    "* What do we need to capture\n",
    "    - Talk title (string)\n",
    "    - Name of presenter(s) (list)\n",
    "    - Description (string)\n",
    "    - Tags (list)\n",
    "    - Link to video (string)\n",
    "    \n",
    "* Let's use JSON \n",
    "    - Awesome for persisting structured data \n",
    "    - Excellent data exchange format\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why namedtuples?\n",
    "* namedtuples are a lightweight container for data\n",
    "* Using a name to access data elements is their advantage\n",
    "* Numeric indexing is all that regular tuple provides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title=Open Data Dashboards & Python Web Scraping\n",
      "names=['Marie Whittaker']\n",
      "tags=['data', 'scraping', 'web']\n",
      "link=https://www.youtube.com/watch?v=kc676iLvib8\n"
     ]
    }
   ],
   "source": [
    "# Storing related data in a namedtuple\n",
    "from collections import namedtuple\n",
    "\n",
    "# create the structure\n",
    "pyvideo = namedtuple('pyvideo', 'title names description tags link')\n",
    "\n",
    "# create an instance\n",
    "talk_data = pyvideo(title=title, names=names, tags=tags, link=link, description=description)\n",
    "\n",
    "# access the data and display it as a formatted string\n",
    "fmt = 'title={}\\nnames={}\\ntags={}\\nlink={}'\n",
    "print(fmt.format(talk_data.title, talk_data.names, talk_data.tags, talk_data.link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why JSON?\n",
    "* Handles structured data well\n",
    "* Excellent data exchange format\n",
    "* Python supports round-tripping between namedtuples and JSON\n",
    "    - namedtuple objects can be saved a JSON formatted text\n",
    "    - JSON formatted text can be convert back to namedtuple objects\n",
    "    - Of course, text can be written to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.pyvideo, __main__.pyvideo)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Writing the namedtuple to disk as JSON\n",
    "import json\n",
    "\n",
    "# Write to file\n",
    "with open('a_video.json', 'w') as fout:\n",
    "    # Note: preserve keys by saving a dictionary, not a list\n",
    "    json.dump(talk_data._asdict(), fout)\n",
    "    \n",
    "# Read from file\n",
    "with open('a_video.json', 'r') as fin:\n",
    "    restored_talk_data = json.load(fin)\n",
    "\n",
    "# Deserialize -- Note: pass as keyword arguments using **\n",
    "restored_talk_data = pyvideo(**restored_talk_data)\n",
    "\n",
    "# Compare semantically\n",
    "talk_data == restored_talk_data\n",
    "type(talk_data), type(restored_talk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Writing web scraping code has advantages\n",
    "\n",
    "* Excellent source of valuable data \n",
    "\n",
    "* The code is approachable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ... and presents some problems too\n",
    "\n",
    "* Your code breaks when the site is updated\n",
    "\n",
    "* Read the terms and conditions, otherwise ...\n",
    "\n",
    "* Getting blocked, banned, sued ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "1. [Automate the Boring Stuff](https://automatetheboringstuff.com/), [Chapter 11 — Web Scraping](https://automatetheboringstuff.com/chapter11/) by Al Sweigart (Free PDF version online).  Takes you through topics step-by-step, includes using Selenium to fill out forms and simulate mouse clicks. \n",
    "\n",
    "1. [RealPython Blog -- Web Scraping With Scrapy and MongoDB](https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb/) by Micheal Herman. Scrapy is a Python package that makes scraping code easier to maintain. \n",
    "\n",
    "1. [Talk Python to Me Podcast -- Web scraping at scale with Scrapy and ScrapingHub](https://talkpython.fm/episodes/show/50/web-scraping-at-scale-with-scrapy-and-scrapinghub). Web scraping as a Service from the author of Scrapy.\n",
    "\n",
    "1. [PyVideo.org](http://pyvideo.org/)— Comprehensive catalog of videos of over 8000 of Python related presentations. Talks on scraping web pages can be found on the [Scraping page](http://pyvideo.org/tag/scraping/). \n",
    "\n",
    "1. [Web Scraping with Python: Collecting Data from the Modern Web](https://www.amazon.com/Web-Scraping-Python-Collecting-Modern/dp/1491910291) by Ryan Mitchell.  This 4.5 star book on Amazon covers scraping topics in depth.\n",
    "\n",
    "1. [Awesome Python](https://awesome-python.com/) -- PyPI has over 100,000 packages.  Awesome Python is a curated list of the best, see their recommended web scraping packages [here](https://awesome-python.com/#web-crawling).\n",
    "\n",
    "1. Other Practice Sites\n",
    "    * [Books to Scrape](http://books.toscrape.com/)\n",
    "    * [Quotes to Scrape](http://quotes.toscrape.com/) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
